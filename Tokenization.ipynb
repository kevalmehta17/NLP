{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c00fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is John.', 'I live in New York City.', 'I love programming in Python.', 'I also enjoy hiking and photography.', \"The weather here is usually great, but it's get quite cold in winter.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"My name is John. I live in New York City. I love programming in Python.\n",
    "I also enjoy hiking and photography. The weather here is usually great, but it's get quite cold in winter.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(corpus, language='english')\n",
    "print(sentences)\n",
    "\n",
    "# sentences are in form of a list\n",
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the sentences\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "554856c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'John',\n",
       " '.',\n",
       " 'I',\n",
       " 'live',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'programming',\n",
       " 'in',\n",
       " 'Python',\n",
       " '.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'enjoy',\n",
       " 'hiking',\n",
       " 'and',\n",
       " 'photography',\n",
       " '.',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'here',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'great',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'quite',\n",
       " 'cold',\n",
       " 'in',\n",
       " 'winter',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------Convert the Paragraph into the words --------------\n",
    "# AND Also the sentences into the words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b766fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'name', 'is', 'John', '.'],\n",
       " ['I', 'live', 'in', 'New', 'York', 'City', '.'],\n",
       " ['I', 'love', 'programming', 'in', 'Python', '.'],\n",
       " ['I', 'also', 'enjoy', 'hiking', 'and', 'photography', '.'],\n",
       " ['The',\n",
       "  'weather',\n",
       "  'here',\n",
       "  'is',\n",
       "  'usually',\n",
       "  'great',\n",
       "  ',',\n",
       "  'but',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'get',\n",
       "  'quite',\n",
       "  'cold',\n",
       "  'in',\n",
       "  'winter',\n",
       "  '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------Lets convert the sentences into words------------------------------\n",
    "# Here we have to loop through the each sentence inside the sentences list \n",
    "sentences_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "sentences_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# Using wordpunct_tokenize to split words and punctuation\n",
    "punct_words = wordpunct_tokenize(corpus)\n",
    "punct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14c2a5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'John.',\n",
       " 'I',\n",
       " 'live',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'programming',\n",
       " 'in',\n",
       " 'Python.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'enjoy',\n",
       " 'hiking',\n",
       " 'and',\n",
       " 'photography.',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'here',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'great',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'quite',\n",
       " 'cold',\n",
       " 'in',\n",
       " 'winter',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Using TreebankWordTokenizer to tokenize the corpus\n",
    "\n",
    "treeban_words = tokenizer.tokenize(corpus)\n",
    "treeban_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
